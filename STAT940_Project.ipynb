{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgBFjTbqa8dz",
        "outputId": "7c80cbf0-0985-48fa-ab1c-a0fdef942095"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "import re\n",
        "import hashlib\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "CcHGEyVJ4g7w"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Load the data"
      ],
      "metadata": {
        "id": "HsRjxq34Z7ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_jsons(folder_path, output_path):\n",
        "    jsons = []\n",
        "    # list to hold contents of jsons\n",
        "    for filename in tqdm(os.listdir(folder_path)):\n",
        "        if filename.endswith('.json'):  # Process only JSON files\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, 'r') as file:\n",
        "                # load json\n",
        "                data = json.load(file)\n",
        "                jsons.append(json.dumps(data))\n",
        "\n",
        "    # combine all jsosn into string\n",
        "    output_content = ',\\n'.join(jsons)\n",
        "    output_content = f\"{output_content}\"\n",
        "\n",
        "    # save combined json\n",
        "    with open(output_path, 'w') as output_file:\n",
        "        output_file.write(output_content)\n",
        "\n",
        "train_folder_path = '/content/drive/MyDrive/Final Project Datasets/data_symbolic_regression/train'\n",
        "train_output_path = '/content/train_merged.json'\n",
        "test_folder_path = '/content/drive/MyDrive/Final Project Datasets/data_symbolic_regression/test'\n",
        "test_output_path = '/content/test_merged.json'\n",
        "val_folder_path = '/content/drive/MyDrive/Final Project Datasets/data_symbolic_regression/val'\n",
        "val_output_path = '/content/val_merged.json'\n",
        "\n",
        "# merge the jsons together because\n",
        "merge_jsons(train_folder_path, train_output_path)\n",
        "merge_jsons(test_folder_path, test_output_path)\n",
        "merge_jsons(val_folder_path, val_output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLhqwF73gWYp",
        "outputId": "6b884530-1012-47ed-c4eb-fd96deb49cd3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 748/748 [00:03<00:00, 243.77it/s]\n",
            "100%|██████████| 162/162 [00:00<00:00, 237.31it/s]\n",
            "100%|██████████| 161/161 [00:00<00:00, 227.30it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Embed the formula"
      ],
      "metadata": {
        "id": "Reajpkyw_22c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_json_path = '/content/train_merged.json'\n",
        "\n",
        "unique_formulas = set()\n",
        "\n",
        "with open(merged_json_path, 'r') as file:\n",
        "        # Read the file line by line\n",
        "        for line in file:\n",
        "            # Remove trailing commas and whitespace\n",
        "            line = line.strip().rstrip(',')\n",
        "\n",
        "            # Parse the line as a JSON object\n",
        "            if line:  # Skip empty lines\n",
        "                try:\n",
        "                    data = json.loads(line)\n",
        "                    if 'formula' in data:\n",
        "                        unique_formulas.add(data['formula'])\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Error decoding line: {line}\\n{e}\")\n",
        "# unique formulas\n",
        "unique_formulas = list(unique_formulas)\n",
        "pprint(unique_formulas[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "y1qcS67birAR",
        "outputId": "e2a40af3-5114-4029-f0d7-47a8ebe53f5d"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mult(cos(add(var_0, var_2), N(N, N)), sqrt(mult(var_1, var_0), N(N, N)))',\n",
            " 'add(add(sqrt(var_2, N), sqrt(var_1, N)), tanh(pow_2(var_0, N), N(N, N)))',\n",
            " 'add(cosh(reverse(var_1, N), N(N, N)), cosh(add(var_0, var_2), N(N, N)))',\n",
            " 'add(tanh(mult(var_1, var_2), N(N, N)), sqrt(pow_2(var_0, N), N(N, N)))',\n",
            " 'add(sinh(pow_2(var_2, N), N(N, N)), sqrt(add(var_0, var_1), N(N, N)))']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to extract tokens from a formula\n",
        "def extract_tokens(formulas):\n",
        "    tokens = set()\n",
        "    for formula in formulas:\n",
        "        tokens.update(re.findall(r\"[a-zA-Z_]\\w*\", formula))\n",
        "    return sorted(tokens)\n",
        "\n",
        "# generate vocabulary\n",
        "vocabulary = extract_tokens(unique_formulas)\n",
        "print(\"Vocabulary:\", vocabulary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qylRTy4jyep",
        "outputId": "c0951cb6-fd5e-487b-92e7-f945c77e31e7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['C_0', 'C_1', 'C_2', 'N', 'add', 'cos', 'cosh', 'exp', 'gaussian', 'log', 'mult', 'neg', 'pow_2', 'reverse', 'sin', 'sinh', 'sqrt', 'tan', 'tanh', 'var_0', 'var_1', 'var_2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I made a lookup table of the equations, later I'll add the equations from the training and validation set so that no possible equations are left out. Maybe we can look for a better way to embed the equations later."
      ],
      "metadata": {
        "id": "-kA_EBOaAYFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "equation_to_index = {eq: idx for idx, eq in enumerate(unique_formulas)}\n",
        "index_to_equation = {idx: eq for eq, idx in equation_to_index.items()}\n",
        "\n",
        "num_equations = len(unique_formulas)\n",
        "embedding_dim = 128  # embedding dimension\n",
        "\n",
        "# embedding layer\n",
        "equation_embeddings = nn.Embedding(num_equations, embedding_dim)\n",
        "\n",
        "# get the embedding of an equation\n",
        "def get_embedding(equation):\n",
        "    index = torch.tensor([equation_to_index[equation]])\n",
        "    embedding = equation_embeddings(index)\n",
        "    return embedding.squeeze(0)\n",
        "\n",
        "# to map back from embedding to equation (since embeddings are unique)\n",
        "def find_equation(embedding):\n",
        "    # create a lookup table\n",
        "    all_embeddings = equation_embeddings.weight.detach()\n",
        "    distances = torch.norm(all_embeddings - embedding, dim=1)\n",
        "    closest_index = torch.argmin(distances).item()\n",
        "    return index_to_equation[closest_index]"
      ],
      "metadata": {
        "id": "GvnLSTjO33W5"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(unique_formulas[0])\n",
        "print(get_embedding(unique_formulas[0]))\n",
        "print(find_equation(get_embedding(unique_formulas[0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKmZaybz4A4-",
        "outputId": "62bf4bec-543f-45b3-8648-d02564f33af2"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mult(cos(add(var_0, var_2), N(N, N)), sqrt(mult(var_1, var_0), N(N, N)))\n",
            "tensor([ 0.1693,  0.6093, -0.8928,  0.4239, -0.1169, -0.5382, -0.8167, -0.6550,\n",
            "        -0.3950,  1.1551,  0.5824,  0.6986,  1.0476, -0.2248,  1.5310, -0.7199,\n",
            "        -0.0892, -1.0237,  0.4608, -0.0519, -1.2601,  1.4363,  0.5297,  0.2367,\n",
            "        -1.4412, -1.0423, -0.4750,  0.0193,  0.7185,  1.4224, -0.6880,  1.0155,\n",
            "        -0.7254, -0.1509,  0.0666, -1.2250,  0.5450,  1.2968, -0.1680, -0.2055,\n",
            "        -0.6902,  0.8764,  0.2365, -0.4168,  1.3870,  0.7504,  1.4949, -1.1196,\n",
            "        -1.6821, -0.2091, -0.6663,  2.0757, -1.3531,  0.4083, -0.7152, -0.8465,\n",
            "        -1.2863, -0.1185, -0.4894,  1.8533, -0.9660, -0.0037, -0.8134,  0.8750,\n",
            "         1.2679,  0.2017, -0.7468, -2.2780, -1.5452, -0.6489, -0.0457,  0.3187,\n",
            "         0.0820,  0.3762, -0.5442,  1.9085, -0.6823,  0.8009,  1.2310, -0.8032,\n",
            "         2.0294,  0.5197, -1.7682, -1.1168,  0.2611, -0.8051, -1.3532, -0.0782,\n",
            "         0.5359,  0.5843, -0.9308,  0.7240, -0.8925, -1.4863, -0.5929, -1.2412,\n",
            "        -0.2394, -0.1362, -0.4737,  0.0699, -0.3380,  1.1932, -0.3960, -0.8990,\n",
            "        -0.3470,  0.4736, -1.8255,  1.1454,  0.5844,  0.7802, -0.8046,  0.1992,\n",
            "        -0.7158, -0.0285, -0.9328,  1.3227,  1.2437, -1.3673, -0.9797, -0.7909,\n",
            "         0.9478,  1.0126, -1.1982, -0.0846,  0.7418, -0.8507,  0.1323, -0.3492],\n",
            "       grad_fn=<SqueezeBackward1>)\n",
            "mult(cos(add(var_0, var_2), N(N, N)), sqrt(mult(var_1, var_0), N(N, N)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Dataset"
      ],
      "metadata": {
        "id": "5ceREQ8p4kBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EquationDataset(Dataset):\n",
        "    def __init__(self, data_point):\n",
        "        self.formula = data_point[\"formula\"]\n",
        "        self.formula_human_readable = data_point[\"formula_human_readable\"]\n",
        "        self.formula_depth = data_point[\"formula_depth\"]\n",
        "        self.n_vars = data_point[\"n_vars\"]\n",
        "        self.n_consts = data_point[\"n_consts\"]\n",
        "        self.n_points = data_point[\"n_points\"]\n",
        "        self.var_bound_dict = data_point[\"var_bound_dict\"]\n",
        "        self.const_value_dict = data_point[\"const_value_dict\"]\n",
        "        self.meta_list = data_point[\"meta_list\"]\n",
        "        self.points = data_point[\"points\"]\n",
        "        self.target = data_point[\"target\"]\n",
        "\n",
        "        self. embedded_formula = get_embedding(self.formula)\n",
        "\n",
        "        # add code here to use the pre-trained t-net to embed the points"
      ],
      "metadata": {
        "id": "6qI26w1f4mOX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}